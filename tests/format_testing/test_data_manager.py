"""
Test Data Manager

Comprehensive test data management system for format testing.
Provides data generation, versioning, validation, and lifecycle management.
"""

import hashlib
import json
import random
import shutil
import sqlite3
import string
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from typing import Any


@dataclass
class FormatTestDataMetadata:
    """Metadata for test data"""

    id: str
    name: str
    description: str
    language: str
    format_types: list[str]
    complexity_level: str  # simple, medium, complex
    file_size_bytes: int
    element_counts: dict[str, int]
    created_timestamp: str
    version: str
    tags: list[str]
    source_hash: str
    validation_status: str  # valid, invalid, unknown


@dataclass
class FormatTestDataSet:
    """Complete test data set"""

    metadata: FormatTestDataMetadata
    source_code: str
    expected_outputs: dict[str, str]  # format_type -> expected_output
    test_scenarios: list[dict[str, Any]]


class FormatTestDataGenerator:
    """Generates test data for various scenarios"""

    def __init__(self):
        self.language_templates = {
            "python": self._get_python_templates(),
            "java": self._get_java_templates(),
            "javascript": self._get_javascript_templates(),
            "typescript": self._get_typescript_templates(),
        }

    def generate_test_data(
        self,
        language: str,
        complexity: str = "medium",
        element_counts: dict[str, int] | None = None,
    ) -> FormatTestDataSet:
        """Generate test data for specified language and complexity"""

        if language not in self.language_templates:
            raise ValueError(f"Unsupported language: {language}")

        # Default element counts based on complexity
        if element_counts is None:
            element_counts = self._get_default_element_counts(complexity)

        # Generate source code
        source_code = self._generate_source_code(language, complexity, element_counts)

        # Generate metadata
        metadata = self._create_metadata(
            language, complexity, element_counts, source_code
        )

        # Generate expected outputs (would be generated by actual analysis)
        expected_outputs = self._generate_expected_outputs(source_code, language)

        # Generate test scenarios
        test_scenarios = self._generate_test_scenarios(language, complexity)

        return FormatTestDataSet(
            metadata=metadata,
            source_code=source_code,
            expected_outputs=expected_outputs,
            test_scenarios=test_scenarios,
        )

    def _get_default_element_counts(self, complexity: str) -> dict[str, int]:
        """Get default element counts based on complexity"""
        counts = {
            "simple": {"classes": 1, "methods": 2, "fields": 1},
            "medium": {"classes": 2, "methods": 5, "fields": 3},
            "complex": {"classes": 5, "methods": 15, "fields": 8},
        }
        return counts.get(complexity, counts["medium"])

    def _generate_source_code(
        self, language: str, complexity: str, element_counts: dict[str, int]
    ) -> str:
        """Generate source code based on specifications"""

        templates = self.language_templates[language]

        if complexity == "simple":
            return templates["simple_class"].format(
                class_name=self._generate_name("Class"),
                method_name=self._generate_name("method"),
                field_name=self._generate_name("field"),
            )

        elif complexity == "medium":
            return templates["medium_class"].format(
                class_name=self._generate_name("Class"),
                base_class=self._generate_name("BaseClass"),
                method1=self._generate_name("method"),
                method2=self._generate_name("method"),
                field1=self._generate_name("field"),
                field2=self._generate_name("field"),
            )

        else:  # complex
            return self._generate_complex_code(language, element_counts)

    def _generate_complex_code(
        self, language: str, element_counts: dict[str, int]
    ) -> str:
        """Generate complex code with specified element counts"""
        templates = self.language_templates[language]

        classes = []
        for _i in range(element_counts.get("classes", 3)):
            class_name = self._generate_name("Class")
            methods = []
            fields = []

            # Generate methods for this class
            methods_per_class = element_counts.get("methods", 10) // element_counts.get(
                "classes", 3
            )
            for _j in range(methods_per_class):
                method_name = self._generate_name("method")
                methods.append(templates["method"].format(method_name=method_name))

            # Generate fields for this class
            fields_per_class = element_counts.get("fields", 6) // element_counts.get(
                "classes", 3
            )
            for _k in range(fields_per_class):
                field_name = self._generate_name("field")
                fields.append(templates["field"].format(field_name=field_name))

            class_code = templates["complex_class"].format(
                class_name=class_name,
                fields="\n    ".join(fields),
                methods="\n    ".join(methods),
            )
            classes.append(class_code)

        return "\n\n".join(classes)

    def _generate_name(self, prefix: str) -> str:
        """Generate random name with prefix"""
        suffix = "".join(random.choices(string.ascii_letters, k=5))
        return f"{prefix}{suffix}"

    def _create_metadata(
        self,
        language: str,
        complexity: str,
        element_counts: dict[str, int],
        source_code: str,
    ) -> FormatTestDataMetadata:
        """Create metadata for test data"""

        source_hash = hashlib.md5(source_code.encode()).hexdigest()
        test_id = f"{language}_{complexity}_{source_hash[:8]}"

        return FormatTestDataMetadata(
            id=test_id,
            name=f"{language.title()} {complexity.title()} Test Data",
            description=f"Generated {complexity} test data for {language}",
            language=language,
            format_types=["full", "compact", "csv"],
            complexity_level=complexity,
            file_size_bytes=len(source_code.encode()),
            element_counts=element_counts,
            created_timestamp=datetime.utcnow().isoformat(),
            version="1.0.0",
            tags=[language, complexity, "generated"],
            source_hash=source_hash,
            validation_status="unknown",
        )

    def _generate_expected_outputs(
        self, source_code: str, language: str
    ) -> dict[str, str]:
        """Generate expected outputs (simplified - would use actual analyzer)"""
        # This is a simplified version - in practice, you'd run the actual analyzer
        return {
            "full": f"# {language.title()} Analysis\n\n## Classes\n\n| Name | Type |\n|------|------|\n| TestClass | class |\n",
            "compact": f"# {language.title()} Info\n\n| Name | Type |\n|------|------|\n| TestClass | class |\n",
            "csv": "Type,Name,ReturnType,Parameters,Access,Static,Final,Line\nclass,TestClass,,,public,false,false,1\n",
        }

    def _generate_test_scenarios(
        self, language: str, complexity: str
    ) -> list[dict[str, Any]]:
        """Generate test scenarios for the data"""
        scenarios = [
            {
                "name": "basic_format_validation",
                "description": "Validate basic format structure",
                "test_type": "format_validation",
                "parameters": {"strict": True},
            },
            {
                "name": "cross_format_consistency",
                "description": "Validate consistency across formats",
                "test_type": "consistency_check",
                "parameters": {"formats": ["full", "compact", "csv"]},
            },
        ]

        if complexity == "complex":
            scenarios.append(
                {
                    "name": "performance_validation",
                    "description": "Validate performance with complex data",
                    "test_type": "performance_test",
                    "parameters": {"max_time_ms": 5000},
                }
            )

        return scenarios

    def _get_python_templates(self) -> dict[str, str]:
        """Get Python code templates"""
        return {
            "simple_class": """class {class_name}:
    def __init__(self):
        self.{field_name} = "value"

    def {method_name}(self):
        return self.{field_name}
""",
            "medium_class": """class {base_class}:
    def __init__(self):
        self.{field1} = "base_value"

class {class_name}({base_class}):
    def __init__(self):
        super().__init__()
        self.{field2} = "derived_value"

    def {method1}(self):
        return self.{field1}

    def {method2}(self):
        return self.{field2}
""",
            "complex_class": """class {class_name}:
    {fields}

    def __init__(self):
        pass

    {methods}
""",
            "method": '''def {method_name}(self):
        return "result"''',
            "field": '''self.{field_name} = "value"''',
        }

    def _get_java_templates(self) -> dict[str, str]:
        """Get Java code templates"""
        return {
            "simple_class": """public class {class_name} {{
    private String {field_name};

    public {class_name}() {{
        this.{field_name} = "value";
    }}

    public String {method_name}() {{
        return this.{field_name};
    }}
}}""",
            "medium_class": """public class {base_class} {{
    protected String {field1};

    public {base_class}() {{
        this.{field1} = "base_value";
    }}
}}

public class {class_name} extends {base_class} {{
    private String {field2};

    public {class_name}() {{
        super();
        this.{field2} = "derived_value";
    }}

    public String {method1}() {{
        return this.{field1};
    }}

    public String {method2}() {{
        return this.{field2};
    }}
}}""",
            "complex_class": """public class {class_name} {{
    {fields}

    public {class_name}() {{
        // Constructor
    }}

    {methods}
}}""",
            "method": """public String {method_name}() {{
        return "result";
    }}""",
            "field": """private String {field_name};""",
        }

    def _get_javascript_templates(self) -> dict[str, str]:
        """Get JavaScript code templates"""
        return {
            "simple_class": """class {class_name} {{
    constructor() {{
        this.{field_name} = "value";
    }}

    {method_name}() {{
        return this.{field_name};
    }}
}}""",
            "medium_class": """class {base_class} {{
    constructor() {{
        this.{field1} = "base_value";
    }}
}}

class {class_name} extends {base_class} {{
    constructor() {{
        super();
        this.{field2} = "derived_value";
    }}

    {method1}() {{
        return this.{field1};
    }}

    {method2}() {{
        return this.{field2};
    }}
}}""",
            "complex_class": """class {class_name} {{
    constructor() {{
        {fields}
    }}

    {methods}
}}""",
            "method": """{method_name}() {{
        return "result";
    }}""",
            "field": """this.{field_name} = "value";""",
        }

    def _get_typescript_templates(self) -> dict[str, str]:
        """Get TypeScript code templates"""
        return {
            "simple_class": """class {class_name} {{
    private {field_name}: string;

    constructor() {{
        this.{field_name} = "value";
    }}

    public {method_name}(): string {{
        return this.{field_name};
    }}
}}""",
            "medium_class": """class {base_class} {{
    protected {field1}: string;

    constructor() {{
        this.{field1} = "base_value";
    }}
}}

class {class_name} extends {base_class} {{
    private {field2}: string;

    constructor() {{
        super();
        this.{field2} = "derived_value";
    }}

    public {method1}(): string {{
        return this.{field1};
    }}

    public {method2}(): string {{
        return this.{field2};
    }}
}}""",
            "complex_class": """class {class_name} {{
    {fields}

    constructor() {{
        // Constructor
    }}

    {methods}
}}""",
            "method": """public {method_name}(): string {{
        return "result";
    }}""",
            "field": """private {field_name}: string;""",
        }


class FormatTestDataRepository:
    """Repository for managing test data storage and retrieval"""

    def __init__(self, repository_path: str = "test_data_repository"):
        self.repository_path = Path(repository_path)
        self.repository_path.mkdir(exist_ok=True)

        self.db_path = self.repository_path / "test_data.db"
        self.data_path = self.repository_path / "data"
        self.data_path.mkdir(exist_ok=True)

        self._init_database()

    def _init_database(self):
        """Initialize test data database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS test_data_sets (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                language TEXT NOT NULL,
                complexity_level TEXT NOT NULL,
                file_size_bytes INTEGER,
                element_counts TEXT,
                created_timestamp TEXT NOT NULL,
                version TEXT NOT NULL,
                tags TEXT,
                source_hash TEXT NOT NULL,
                validation_status TEXT DEFAULT 'unknown',
                file_path TEXT NOT NULL
            )
        """
        )

        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS test_data_usage (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_data_id TEXT NOT NULL,
                usage_timestamp TEXT NOT NULL,
                test_type TEXT NOT NULL,
                result TEXT,
                FOREIGN KEY (test_data_id) REFERENCES test_data_sets (id)
            )
        """
        )

        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS test_data_versions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_data_id TEXT NOT NULL,
                version TEXT NOT NULL,
                changes TEXT,
                created_timestamp TEXT NOT NULL,
                FOREIGN KEY (test_data_id) REFERENCES test_data_sets (id)
            )
        """
        )

        conn.commit()
        conn.close()

    def store_test_data(self, test_data_set: FormatTestDataSet) -> str:
        """Store test data set in repository"""
        # Create data directory for this test set
        data_dir = self.data_path / test_data_set.metadata.id
        data_dir.mkdir(exist_ok=True)

        # Save source code
        source_file = (
            data_dir
            / f"source.{self._get_file_extension(test_data_set.metadata.language)}"
        )
        with open(source_file, "w", encoding="utf-8") as f:
            f.write(test_data_set.source_code)

        # Save expected outputs
        outputs_dir = data_dir / "expected_outputs"
        outputs_dir.mkdir(exist_ok=True)

        for format_type, output in test_data_set.expected_outputs.items():
            output_file = outputs_dir / f"{format_type}.txt"
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(output)

        # Save metadata and scenarios
        metadata_file = data_dir / "metadata.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "metadata": asdict(test_data_set.metadata),
                    "test_scenarios": test_data_set.test_scenarios,
                },
                f,
                indent=2,
            )

        # Store in database
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute(
            """
            INSERT OR REPLACE INTO test_data_sets
            (id, name, description, language, complexity_level, file_size_bytes,
             element_counts, created_timestamp, version, tags, source_hash,
             validation_status, file_path)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                test_data_set.metadata.id,
                test_data_set.metadata.name,
                test_data_set.metadata.description,
                test_data_set.metadata.language,
                test_data_set.metadata.complexity_level,
                test_data_set.metadata.file_size_bytes,
                json.dumps(test_data_set.metadata.element_counts),
                test_data_set.metadata.created_timestamp,
                test_data_set.metadata.version,
                json.dumps(test_data_set.metadata.tags),
                test_data_set.metadata.source_hash,
                test_data_set.metadata.validation_status,
                str(data_dir),
            ),
        )

        conn.commit()
        conn.close()

        return test_data_set.metadata.id

    def get_test_data(self, test_data_id: str) -> FormatTestDataSet | None:
        """Retrieve test data set by ID"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute(
            """
            SELECT * FROM test_data_sets WHERE id = ?
        """,
            (test_data_id,),
        )

        row = cursor.fetchone()
        conn.close()

        if not row:
            return None

        # Load from files
        data_dir = Path(row[12])  # file_path column

        # Load source code
        language = row[3]
        source_file = data_dir / f"source.{self._get_file_extension(language)}"
        with open(source_file, encoding="utf-8") as f:
            source_code = f.read()

        # Load expected outputs
        expected_outputs = {}
        outputs_dir = data_dir / "expected_outputs"
        if outputs_dir.exists():
            for output_file in outputs_dir.glob("*.txt"):
                format_type = output_file.stem
                with open(output_file, encoding="utf-8") as f:
                    expected_outputs[format_type] = f.read()

        # Load metadata and scenarios
        metadata_file = data_dir / "metadata.json"
        with open(metadata_file, encoding="utf-8") as f:
            data = json.load(f)
            metadata = FormatTestDataMetadata(**data["metadata"])
            test_scenarios = data["test_scenarios"]

        return FormatTestDataSet(
            metadata=metadata,
            source_code=source_code,
            expected_outputs=expected_outputs,
            test_scenarios=test_scenarios,
        )

    def search_test_data(
        self,
        language: str | None = None,
        complexity: str | None = None,
        tags: list[str] | None = None,
        limit: int = 100,
    ) -> list[FormatTestDataMetadata]:
        """Search for test data sets"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        query = "SELECT * FROM test_data_sets WHERE 1=1"
        params = []

        if language:
            query += " AND language = ?"
            params.append(language)

        if complexity:
            query += " AND complexity_level = ?"
            params.append(complexity)

        if tags:
            for tag in tags:
                query += " AND tags LIKE ?"
                params.append(f"%{tag}%")

        query += f" ORDER BY created_timestamp DESC LIMIT {limit}"

        cursor.execute(query, params)
        rows = cursor.fetchall()
        conn.close()

        results = []
        for row in rows:
            metadata = FormatTestDataMetadata(
                id=row[0],
                name=row[1],
                description=row[2],
                language=row[3],
                format_types=["full", "compact", "csv"],  # Default
                complexity_level=row[4],
                file_size_bytes=row[5],
                element_counts=json.loads(row[6]),
                created_timestamp=row[7],
                version=row[8],
                tags=json.loads(row[9]),
                source_hash=row[10],
                validation_status=row[11],
            )
            results.append(metadata)

        return results

    def record_usage(self, test_data_id: str, test_type: str, result: str):
        """Record test data usage"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute(
            """
            INSERT INTO test_data_usage
            (test_data_id, usage_timestamp, test_type, result)
            VALUES (?, ?, ?, ?)
        """,
            (test_data_id, datetime.utcnow().isoformat(), test_type, result),
        )

        conn.commit()
        conn.close()

    def get_usage_statistics(self) -> dict[str, Any]:
        """Get test data usage statistics"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # Total test data sets
        cursor.execute("SELECT COUNT(*) FROM test_data_sets")
        total_sets = cursor.fetchone()[0]

        # By language
        cursor.execute(
            """
            SELECT language, COUNT(*) FROM test_data_sets
            GROUP BY language
        """
        )
        by_language = dict(cursor.fetchall())

        # By complexity
        cursor.execute(
            """
            SELECT complexity_level, COUNT(*) FROM test_data_sets
            GROUP BY complexity_level
        """
        )
        by_complexity = dict(cursor.fetchall())

        # Usage frequency
        cursor.execute(
            """
            SELECT test_data_id, COUNT(*) as usage_count
            FROM test_data_usage
            GROUP BY test_data_id
            ORDER BY usage_count DESC
            LIMIT 10
        """
        )
        most_used = cursor.fetchall()

        conn.close()

        return {
            "total_test_sets": total_sets,
            "by_language": by_language,
            "by_complexity": by_complexity,
            "most_used": most_used,
        }

    def _get_file_extension(self, language: str) -> str:
        """Get file extension for language"""
        extensions = {
            "python": "py",
            "java": "java",
            "javascript": "js",
            "typescript": "ts",
            "html": "html",
            "css": "css",
        }
        return extensions.get(language, "txt")


class TestDataValidator:
    """Validates test data quality and consistency"""

    def validate_test_data_set(
        self, test_data_set: FormatTestDataSet
    ) -> list[dict[str, Any]]:
        """Validate test data set quality"""
        issues = []

        # Validate metadata
        issues.extend(self._validate_metadata(test_data_set.metadata))

        # Validate source code
        issues.extend(
            self._validate_source_code(
                test_data_set.source_code, test_data_set.metadata.language
            )
        )

        # Validate expected outputs
        issues.extend(self._validate_expected_outputs(test_data_set.expected_outputs))

        # Validate consistency
        issues.extend(self._validate_consistency(test_data_set))

        return issues

    def _validate_metadata(
        self, metadata: FormatTestDataMetadata
    ) -> list[dict[str, Any]]:
        """Validate metadata quality"""
        issues = []

        if not metadata.name:
            issues.append(
                {
                    "type": "metadata_error",
                    "message": "Test data name is required",
                    "severity": "error",
                }
            )

        if not metadata.description:
            issues.append(
                {
                    "type": "metadata_warning",
                    "message": "Test data description is missing",
                    "severity": "warning",
                }
            )

        if metadata.language not in [
            "python",
            "java",
            "javascript",
            "typescript",
            "html",
            "css",
        ]:
            issues.append(
                {
                    "type": "metadata_error",
                    "message": f"Unsupported language: {metadata.language}",
                    "severity": "error",
                }
            )

        return issues

    def _validate_source_code(
        self, source_code: str, language: str
    ) -> list[dict[str, Any]]:
        """Validate source code quality"""
        issues = []

        if not source_code.strip():
            issues.append(
                {
                    "type": "source_error",
                    "message": "Source code is empty",
                    "severity": "error",
                }
            )
            return issues

        # Basic syntax validation
        if language == "python":
            try:
                compile(source_code, "<test>", "exec")
            except SyntaxError as e:
                issues.append(
                    {
                        "type": "syntax_error",
                        "message": f"Python syntax error: {e}",
                        "severity": "error",
                    }
                )

        # Check for minimum complexity
        lines = source_code.split("\n")
        if len(lines) < 5:
            issues.append(
                {
                    "type": "complexity_warning",
                    "message": "Source code may be too simple for comprehensive testing",
                    "severity": "warning",
                }
            )

        return issues

    def _validate_expected_outputs(
        self, expected_outputs: dict[str, str]
    ) -> list[dict[str, Any]]:
        """Validate expected outputs"""
        issues = []

        required_formats = ["full", "compact", "csv"]
        for format_type in required_formats:
            if format_type not in expected_outputs:
                issues.append(
                    {
                        "type": "output_error",
                        "message": f"Missing expected output for format: {format_type}",
                        "severity": "error",
                    }
                )
            elif not expected_outputs[format_type].strip():
                issues.append(
                    {
                        "type": "output_error",
                        "message": f"Empty expected output for format: {format_type}",
                        "severity": "error",
                    }
                )

        return issues

    def _validate_consistency(
        self, test_data_set: FormatTestDataSet
    ) -> list[dict[str, Any]]:
        """Validate internal consistency"""
        issues = []

        # Check if element counts match source code
        actual_counts = self._count_elements_in_source(
            test_data_set.source_code, test_data_set.metadata.language
        )

        expected_counts = test_data_set.metadata.element_counts

        for element_type, expected_count in expected_counts.items():
            actual_count = actual_counts.get(element_type, 0)
            if actual_count != expected_count:
                issues.append(
                    {
                        "type": "consistency_warning",
                        "message": f"Element count mismatch for {element_type}: expected {expected_count}, found {actual_count}",
                        "severity": "warning",
                    }
                )

        return issues

    def _count_elements_in_source(
        self, source_code: str, language: str
    ) -> dict[str, int]:
        """Count elements in source code"""
        counts = {"classes": 0, "methods": 0, "fields": 0}

        if language == "python":
            import ast

            try:
                tree = ast.parse(source_code)
                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        counts["classes"] += 1
                    elif isinstance(node, ast.FunctionDef):
                        counts["methods"] += 1
                    elif isinstance(node, ast.Assign):
                        counts["fields"] += len(node.targets)
            except (SyntaxError, ValueError):
                pass

        elif language == "java":
            import re

            counts["classes"] = len(re.findall(r"\bclass\s+\w+", source_code))
            counts["methods"] = len(re.findall(r"\w+\s*\([^)]*\)\s*{", source_code))
            counts["fields"] = len(
                re.findall(r"private|public|protected.*?;", source_code)
            )

        return counts


class FormatTestDataManager:
    """Main interface for test data management"""

    def __init__(self, repository_path: str = "test_data_repository"):
        self.generator = FormatTestDataGenerator()
        self.repository = FormatTestDataRepository(repository_path)
        self.validator = TestDataValidator()

    def create_test_data_suite(
        self,
        languages: list[str],
        complexities: list[str] = None,
        count_per_combination: int = 3,
    ) -> list[str]:
        """Create comprehensive test data suite"""
        if complexities is None:
            complexities = ["simple", "medium", "complex"]

        created_ids = []

        for language in languages:
            for complexity in complexities:
                for i in range(count_per_combination):
                    try:
                        # Generate test data
                        test_data = self.generator.generate_test_data(
                            language, complexity
                        )

                        # Validate test data
                        issues = self.validator.validate_test_data_set(test_data)

                        # Only store if no critical issues
                        critical_issues = [
                            i for i in issues if i["severity"] == "error"
                        ]
                        if not critical_issues:
                            test_id = self.repository.store_test_data(test_data)
                            created_ids.append(test_id)
                        else:
                            print(
                                f"Skipping test data due to critical issues: {critical_issues}"
                            )

                    except Exception as e:
                        print(
                            f"Error creating test data for {language}/{complexity}: {e}"
                        )
                        continue

        return created_ids

    def get_test_data_for_scenario(
        self, language: str, complexity: str = "medium", scenario_type: str = "basic"
    ) -> FormatTestDataSet | None:
        """Get test data suitable for specific testing scenario"""

        # Search for existing test data
        candidates = self.repository.search_test_data(
            language=language, complexity=complexity, limit=10
        )

        if candidates:
            # Return the first suitable candidate
            return self.repository.get_test_data(candidates[0].id)

        # Generate new test data if none found
        test_data = self.generator.generate_test_data(language, complexity)
        self.repository.store_test_data(test_data)
        return test_data

    def validate_repository(self) -> dict[str, Any]:
        """Validate entire test data repository"""
        all_metadata = self.repository.search_test_data(limit=1000)

        validation_results = {
            "total_sets": len(all_metadata),
            "valid_sets": 0,
            "invalid_sets": 0,
            "issues_by_type": {},
            "detailed_issues": [],
        }

        for metadata in all_metadata:
            test_data = self.repository.get_test_data(metadata.id)
            if test_data:
                issues = self.validator.validate_test_data_set(test_data)

                if not issues:
                    validation_results["valid_sets"] += 1
                else:
                    validation_results["invalid_sets"] += 1

                    for issue in issues:
                        issue_type = issue["type"]
                        if issue_type not in validation_results["issues_by_type"]:
                            validation_results["issues_by_type"][issue_type] = 0
                        validation_results["issues_by_type"][issue_type] += 1

                        validation_results["detailed_issues"].append(
                            {"test_data_id": metadata.id, "issue": issue}
                        )

        return validation_results

    def cleanup_invalid_data(self) -> int:
        """Remove invalid test data from repository"""
        all_metadata = self.repository.search_test_data(limit=1000)
        removed_count = 0

        for metadata in all_metadata:
            test_data = self.repository.get_test_data(metadata.id)
            if test_data:
                issues = self.validator.validate_test_data_set(test_data)
                critical_issues = [i for i in issues if i["severity"] == "error"]

                if critical_issues:
                    # Remove invalid test data
                    self._remove_test_data(metadata.id)
                    removed_count += 1

        return removed_count

    def _remove_test_data(self, test_data_id: str):
        """Remove test data from repository"""
        # Get file path
        conn = sqlite3.connect(self.repository.db_path)
        cursor = conn.cursor()

        cursor.execute(
            "SELECT file_path FROM test_data_sets WHERE id = ?", (test_data_id,)
        )
        row = cursor.fetchone()

        if row:
            file_path = Path(row[0])
            if file_path.exists():
                shutil.rmtree(file_path)

            # Remove from database
            cursor.execute("DELETE FROM test_data_sets WHERE id = ?", (test_data_id,))
            cursor.execute(
                "DELETE FROM test_data_usage WHERE test_data_id = ?", (test_data_id,)
            )
            cursor.execute(
                "DELETE FROM test_data_versions WHERE test_data_id = ?", (test_data_id,)
            )

        conn.commit()
        conn.close()

    def export_test_data_suite(
        self, output_path: str, filters: dict[str, Any] | None = None
    ) -> str:
        """Export test data suite for sharing or backup"""
        output_path = Path(output_path)
        output_path.mkdir(exist_ok=True)

        # Search for test data based on filters
        if filters:
            test_data_list = self.repository.search_test_data(
                language=filters.get("language"),
                complexity=filters.get("complexity"),
                tags=filters.get("tags"),
                limit=filters.get("limit", 1000),
            )
        else:
            test_data_list = self.repository.search_test_data(limit=1000)

        # Export each test data set
        exported_count = 0
        for metadata in test_data_list:
            test_data = self.repository.get_test_data(metadata.id)
            if test_data:
                # Create export directory for this test set
                export_dir = output_path / metadata.id
                export_dir.mkdir(exist_ok=True)

                # Export source code
                source_file = (
                    export_dir
                    / f"source.{self.repository._get_file_extension(metadata.language)}"
                )
                with open(source_file, "w", encoding="utf-8") as f:
                    f.write(test_data.source_code)

                # Export expected outputs
                outputs_dir = export_dir / "expected_outputs"
                outputs_dir.mkdir(exist_ok=True)

                for format_type, output in test_data.expected_outputs.items():
                    output_file = outputs_dir / f"{format_type}.txt"
                    with open(output_file, "w", encoding="utf-8") as f:
                        f.write(output)

                # Export metadata
                metadata_file = export_dir / "metadata.json"
                with open(metadata_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "metadata": asdict(test_data.metadata),
                            "test_scenarios": test_data.test_scenarios,
                        },
                        f,
                        indent=2,
                    )

                exported_count += 1

        # Create export manifest
        manifest_file = output_path / "export_manifest.json"
        with open(manifest_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "export_timestamp": datetime.utcnow().isoformat(),
                    "exported_count": exported_count,
                    "filters_applied": filters or {},
                    "test_data_ids": [m.id for m in test_data_list],
                },
                f,
                indent=2,
            )

        return str(output_path)

    def import_test_data_suite(self, import_path: str) -> dict[str, Any]:
        """Import test data suite from export"""
        import_path = Path(import_path)

        if not import_path.exists():
            raise ValueError(f"Import path does not exist: {import_path}")

        manifest_file = import_path / "export_manifest.json"
        if not manifest_file.exists():
            raise ValueError("Export manifest not found")

        with open(manifest_file, encoding="utf-8") as f:
            manifest = json.load(f)

        imported_count = 0
        skipped_count = 0
        errors = []

        for test_data_id in manifest["test_data_ids"]:
            test_dir = import_path / test_data_id

            if not test_dir.exists():
                errors.append(f"Test data directory not found: {test_data_id}")
                continue

            try:
                # Load metadata
                metadata_file = test_dir / "metadata.json"
                with open(metadata_file, encoding="utf-8") as f:
                    data = json.load(f)
                    metadata = FormatTestDataMetadata(**data["metadata"])
                    test_scenarios = data["test_scenarios"]

                # Check if already exists
                existing = self.repository.get_test_data(test_data_id)
                if existing:
                    skipped_count += 1
                    continue

                # Load source code
                source_file = (
                    test_dir
                    / f"source.{self.repository._get_file_extension(metadata.language)}"
                )
                with open(source_file, encoding="utf-8") as f:
                    source_code = f.read()

                # Load expected outputs
                expected_outputs = {}
                outputs_dir = test_dir / "expected_outputs"
                if outputs_dir.exists():
                    for output_file in outputs_dir.glob("*.txt"):
                        format_type = output_file.stem
                        with open(output_file, encoding="utf-8") as f:
                            expected_outputs[format_type] = f.read()

                # Create test data set
                test_data_set = FormatTestDataSet(
                    metadata=metadata,
                    source_code=source_code,
                    expected_outputs=expected_outputs,
                    test_scenarios=test_scenarios,
                )

                # Store in repository
                self.repository.store_test_data(test_data_set)
                imported_count += 1

            except Exception as e:
                errors.append(f"Error importing {test_data_id}: {e}")

        return {
            "imported_count": imported_count,
            "skipped_count": skipped_count,
            "errors": errors,
            "total_in_manifest": len(manifest["test_data_ids"]),
        }
